{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfaiss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai_vpt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m PI_HEAD_KWARGS, MineRLAgent\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai_vpt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicy\u001b[39;00m \u001b[39mimport\u001b[39;00m MinecraftPolicy\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import gym\n",
    "import minerl\n",
    "import torch\n",
    "import torch as th\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "# import faiss\n",
    "\n",
    "from openai_vpt.agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "from openai_vpt.lib.policy import MinecraftPolicy\n",
    "from data_loader import DataLoader\n",
    "from openai_vpt.lib.tree_util import tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(\"data/MakeWaterfallTrain/*.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USING_FULL_DATASET = False\n",
    "\n",
    "EPOCHS = 1 if USING_FULL_DATASET else 1\n",
    "# Needs to be <= number of videos\n",
    "BATCH_SIZE = 64 if USING_FULL_DATASET else 16\n",
    "# Ideally more than batch size to create\n",
    "# variation in datasets (otherwise, you will\n",
    "# get a bunch of consecutive samples)\n",
    "# Decrease this (and batch_size) if you run out of memory\n",
    "N_WORKERS = 100 if USING_FULL_DATASET else 16\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "LOSS_REPORT_RATE = 100\n",
    "\n",
    "# Tuned with bit of trial and error\n",
    "LEARNING_RATE = 0.000181\n",
    "# OpenAI VPT BC weight decay\n",
    "# WEIGHT_DECAY = 0.039428\n",
    "WEIGHT_DECAY = 0.0\n",
    "# KL loss to the original model was not used in OpenAI VPT\n",
    "KL_LOSS_WEIGHT = 1.0\n",
    "MAX_GRAD_NORM = 5.0\n",
    "\n",
    "MAX_BATCHES = 2000 if USING_FULL_DATASET else int(1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_parameters(path_to_model_file):\n",
    "    agent_parameters = pickle.load(open(path_to_model_file, \"rb\"))\n",
    "    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "    return policy_kwargs, pi_head_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_model = \"data/VPT-models/foundation-model-1x.model\"\n",
    "in_weights = \"data/VPT-models/foundation-model-1x.weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs, pi_head_kwargs = load_model_parameters(in_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs['recurrence_type'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpt_cnn = MinecraftPolicy(**policy_kwargs, single_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpt_cnn.recurrent_layer = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinecraftPolicy(\n",
       "  (img_preprocess): ImgPreprocessing()\n",
       "  (img_process): ImgObsProcess(\n",
       "    (cnn): ImpalaCNN(\n",
       "      (stacks): ModuleList(\n",
       "        (0): CnnDownStack(\n",
       "          (firstconv): FanInInitReLULayer(\n",
       "            (layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (n): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (blocks): ModuleList(\n",
       "            (0): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CnnDownStack(\n",
       "          (firstconv): FanInInitReLULayer(\n",
       "            (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (n): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (blocks): ModuleList(\n",
       "            (0): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CnnDownStack(\n",
       "          (firstconv): FanInInitReLULayer(\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (n): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (blocks): ModuleList(\n",
       "            (0): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dense): FanInInitReLULayer(\n",
       "        (norm): LayerNorm((32768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer): Linear(in_features=32768, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): FanInInitReLULayer(\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer): Linear(in_features=256, out_features=1024, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (recurrent_layer): Identity()\n",
       "  (lastlayer): FanInInitReLULayer(\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  )\n",
       "  (final_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpt_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MineRLAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agent_policy_kwargs, agent_pi_head_kwargs \u001b[39m=\u001b[39m load_model_parameters(in_model)\n\u001b[0;32m----> 3\u001b[0m agent \u001b[39m=\u001b[39m MineRLAgent(device\u001b[39m=\u001b[39mDEVICE, policy_kwargs\u001b[39m=\u001b[39magent_policy_kwargs, pi_head_kwargs\u001b[39m=\u001b[39magent_pi_head_kwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MineRLAgent' is not defined"
     ]
    }
   ],
   "source": [
    "agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(in_model)\n",
    "\n",
    "agent = MineRLAgent(device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictActionHead(\n",
       "  (camera): CategoricalActionHead(\n",
       "    (linear_layer): Linear(in_features=1024, out_features=121, bias=True)\n",
       "  )\n",
       "  (buttons): CategoricalActionHead(\n",
       "    (linear_layer): Linear(in_features=1024, out_features=8641, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.pi_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_vpt.lib.action_mapping import CameraHierarchicalMapping\n",
    "from openai_vpt.lib.actions import ActionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_TRANSFORMER_KWARGS = dict(\n",
    "    camera_binsize=2,\n",
    "    camera_maxval=10,\n",
    "    camera_mu=10,\n",
    "    camera_quantization_scheme=\"mu_law\",\n",
    ")\n",
    "action_mapper = CameraHierarchicalMapping(n_camera_bins=11)\n",
    "action_transformer = ActionTransformer(**ACTION_TRANSFORMER_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_action_to_agent(minerl_action_transformed, to_torch=False, check_if_null=False):\n",
    "    \"\"\"\n",
    "    Turn action from MineRL to model's action.\n",
    "\n",
    "    Note that this will add batch dimensions to the action.\n",
    "    Returns numpy arrays, unless `to_torch` is True, in which case it returns torch tensors.\n",
    "\n",
    "    If `check_if_null` is True, check if the action is null (no action) after the initial\n",
    "    transformation. This matches the behaviour done in OpenAI's VPT work.\n",
    "    If action is null, return \"None\" instead\n",
    "    \"\"\"\n",
    "    minerl_action = action_transformer.env2policy(minerl_action_transformed)\n",
    "    if check_if_null:\n",
    "        if np.all(minerl_action[\"buttons\"] == 0) and np.all(minerl_action[\"camera\"] == action_transformer.camera_zero_bin):\n",
    "            return None\n",
    "\n",
    "    # Add batch dims if not existant\n",
    "    if minerl_action[\"camera\"].ndim == 1:\n",
    "        minerl_action = {k: v[None] for k, v in minerl_action.items()}\n",
    "    action = action_mapper.from_factored(minerl_action)\n",
    "    if to_torch:\n",
    "        action = {k: th.from_numpy(v).to(DEVICE) for k, v in action.items()}\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"data/MakeWaterfall/Player571-f153ac423f61-20220707-110239.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames(video_path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(jsonl_path):\n",
    "    with open(jsonl_path) as f:\n",
    "        return [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = load_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_jsonl(\"data/MakeWaterfall/Player571-f153ac423f61-20220707-110239.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(frames):\n",
    "    for frame in frames:\n",
    "        cv2.imshow(\"image\", frame)\n",
    "        cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = glob.glob(os.path.join('data/MakeWaterfall/', \"*.mp4\"))\n",
    "unique_ids = list(set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_first = th.from_numpy(np.array((False,))).to(DEVICE).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPTModel():\n",
    "    '''VPT Model for Embedding Situations and Single Observations'''\n",
    "\n",
    "    def __init__(self, model_path, weights_path=None, freeze=True, device='auto'):\n",
    "\n",
    "        agent_policy_kwargs = self.load_model_parameters(model_path)\n",
    "        self.policy = MinecraftPolicy(\n",
    "            **agent_policy_kwargs, single_output=True)\n",
    "\n",
    "        if device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.policy.to(self.device)\n",
    "\n",
    "        if weights_path is not None:\n",
    "            self.policy.load_state_dict(torch.load(\n",
    "                weights_path, map_location=self.device))\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.policy.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dummy_first = torch.from_numpy(np.array((False,))).to(self.device).unsqueeze(1)\n",
    "\n",
    "    def load_model_parameters(self, model_path):\n",
    "        '''Load model parameters from model_path'''\n",
    "\n",
    "        with open(model_path, 'rb') as f:\n",
    "            agent_parameters = pickle.load(f)\n",
    "            policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "\n",
    "            return policy_kwargs\n",
    "\n",
    "    def preprocess_obs(self, obs_frame):\n",
    "        '''Turn observation from MineRL environment into model's observation'''\n",
    "        policy_input = cv2.resize(\n",
    "            obs_frame, (128, 128), interpolation=cv2.INTER_LINEAR)[None]\n",
    "        policy_input = {\"img\": torch.from_numpy(policy_input).to(self.device)}\n",
    "        return policy_input\n",
    "    \n",
    "    def encode(self, obs, state_in):\n",
    "        '''Encode observation into latent space'''\n",
    "\n",
    "        obs = self.preprocess_obs(obs)\n",
    "        obs = tree_map(lambda x: x.unsqueeze(1), obs)\n",
    "        latent_vec, state_out = self.policy(obs, state_in, context={\"first\": self.dummy_first})\n",
    "        \n",
    "        return latent_vec, state_out\n",
    "    \n",
    "    def encode_trajectory(self, trajectory):\n",
    "        '''Encode expert trajectory frames into a latent vector with state history'''\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            initial_state = self.policy.initial_state(1)\n",
    "            hidden_state = initial_state\n",
    "            latent_vectors = []\n",
    "\n",
    "            for obs in tqdm(trajectory):\n",
    "                latent, state_out = self.encode(obs, hidden_state)\n",
    "                hidden_state = state_out\n",
    "                latent_vectors.append(latent.squeeze().detach().cpu().numpy())\n",
    "\n",
    "            return latent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SituationsLoader():\n",
    "    '''Load the data from the MakeWaterfall dataset and create situations'''\n",
    "    def __init__(self, data_dir='data/MakeWaterfall/'):\n",
    "        unique_ids = glob.glob(os.path.join(data_dir, \"*.mp4\"))\n",
    "        unique_ids = list(set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))[:1]\n",
    "\n",
    "        self.demonstration_tuples = []\n",
    "        for unique_id in unique_ids:\n",
    "            video_path = os.path.abspath(os.path.join(data_dir, unique_id + \".mp4\"))\n",
    "            json_path = os.path.abspath(os.path.join(data_dir, unique_id + \".jsonl\"))\n",
    "            self.demonstration_tuples.append((unique_id, video_path, json_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.demonstration_tuples)\n",
    "    \n",
    "    def _load_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def _load_jsonl(self, jsonl_path):\n",
    "        with open(jsonl_path) as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "        \n",
    "    def build_situations(self, window_size=128, stride=2):\n",
    "        \n",
    "        situations = []\n",
    "        for unique_id, video_path, json_path in tqdm(self.demonstration_tuples):\n",
    "            video = self._load_video(video_path)\n",
    "            jsonl = self._load_jsonl(json_path)\n",
    "            \n",
    "            for i in range(window_size, len(video) - window_size, stride):\n",
    "                situation = {}\n",
    "                situation['demo_id'] = unique_id\n",
    "                situation['situation_idx'] = i\n",
    "                situation['situation_obs'] = video[i-window_size:i+1] # 128 context + 1 current\n",
    "                situations.append(situation)\n",
    "                \n",
    "        return situations\n",
    "    \n",
    "    def load_demonstrations(self):\n",
    "        demonstrations = []\n",
    "        for unique_id, video_path, json_path in tqdm(self.demonstration_tuples):\n",
    "            video = self._load_video(video_path)\n",
    "            jsonl = self._load_jsonl(json_path)\n",
    "            demonstrations.append({\n",
    "                'demo_id': unique_id,\n",
    "                'video': video,\n",
    "                'jsonl': jsonl\n",
    "\n",
    "            })\n",
    "        return demonstrations\n",
    "    \n",
    "    def save_situations(self, situations, save_path):\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(situations, f)\n",
    "\n",
    "    def load_situations(self, save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SituationsLoader2():\n",
    "    '''Complete External Memory System for REBECA'''\n",
    "\n",
    "    def __init__(self, model_path, weights_path=None, freeze=True, device='auto'):\n",
    "        self.vpt = VPTModel(model_path, weights_path, freeze, device)\n",
    "\n",
    "    def load_expert_data(self, data_dir='data/MakeWaterfall/'):\n",
    "        '''Load expert demonstrations from data_dir'''\n",
    "\n",
    "        unique_ids = glob.glob(os.path.join(data_dir, \"*.mp4\"))\n",
    "        unique_ids = list(\n",
    "            set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))[:1]\n",
    "\n",
    "        self.demonstration_tuples = []\n",
    "        for unique_id in unique_ids:\n",
    "            video_path = os.path.abspath(\n",
    "                os.path.join(data_dir, unique_id + \".mp4\"))\n",
    "            json_path = os.path.abspath(\n",
    "                os.path.join(data_dir, unique_id + \".jsonl\"))\n",
    "            self.demonstration_tuples.append(\n",
    "                (unique_id, video_path, json_path))\n",
    "            \n",
    "    def load_demonstrations(self):\n",
    "        demonstrations = []\n",
    "        for unique_id, video_path, json_path in tqdm(self.demonstration_tuples):\n",
    "            video = self._load_video(video_path)\n",
    "            jsonl = self._load_jsonl(json_path)\n",
    "            demonstrations.append({\n",
    "                'demo_id': unique_id,\n",
    "                'video': video,\n",
    "                'jsonl': jsonl\n",
    "\n",
    "            })\n",
    "        return demonstrations\n",
    "    \n",
    "    def encode_demonstrations(self, demonstrations):\n",
    "        encoded_demos = []\n",
    "        for demo in tqdm(demonstrations):\n",
    "            encoded_demo = self.vpt.encode_trajectory(demo['video'])\n",
    "            encoded_demos.append({\n",
    "                'demo_id': demo['demo_id'],\n",
    "                'encoded_demo': encoded_demo\n",
    "            })\n",
    "        return encoded_demos\n",
    "    \n",
    "    def create_situations(self, encoded_demos, window_size=128, stride=2):\n",
    "        situations = []\n",
    "        for demo in tqdm(encoded_demos):\n",
    "            for i in range(window_size, len(demo['encoded_demo']) - window_size, stride):\n",
    "                situations.append({\n",
    "                    'demo_id': demo['demo_id'],\n",
    "                    'situation_idx': i,\n",
    "                    'situation': demo['encoded_demo'][i]\n",
    "                })\n",
    "        return situations\n",
    "\n",
    "    def _load_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (128, 128),\n",
    "                               interpolation=cv2.INTER_LINEAR)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def _load_jsonl(self, jsonl_path):\n",
    "        with open(jsonl_path) as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "\n",
    "    def save_situations(self, situations, save_path):\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(situations, f)\n",
    "\n",
    "    def load_situations(self, save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_loader = SituationsLoader()\n",
    "memory = Memory(in_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_expert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff01f5f78cd4855858baff3dbac3f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demonstrations = memory.load_demonstrations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f57ab1a97174397b9ed0ed6f165870f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752f189032c64c0e95593fec31d1aa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255defc0c8554b3fab0ceaf0d803c27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_demos = memory.encode_demonstrations(demonstrations)\n",
    "mem_situations = memory.create_situations(encoded_demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mem_situations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7332349bc4844275b744fdd92ebb910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "situations = situation_loader.build_situations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 49923)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(situations), len(situations) * 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf6f8b9217b4d8e9b3f1c671fc26959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demonstrations = situation_loader.load_demonstrations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(demo['video']) for demo in demonstrations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50e5e0a83be4c81ac7b5a33391f1910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "situation_latents = []\n",
    "\n",
    "with th.inference_mode():\n",
    "    for situation in tqdm(situations[:3]):\n",
    "        situation_obs = situation['situation_obs']\n",
    "\n",
    "        initial_state = memory.vpt.policy.initial_state(1)\n",
    "        states = [initial_state]\n",
    "\n",
    "        for obs in situation_obs:\n",
    "            obs = memory.vpt.preprocess_obs(obs)\n",
    "            obs = tree_map(lambda x: x.unsqueeze(1), obs)\n",
    "            pi_latent, state_out = memory.vpt.policy(obs, states[-1], context={\"first\": dummy_first})\n",
    "            states.append(state_out)\n",
    "        \n",
    "        situation_latents.append({\n",
    "            'demo_id': situation['demo_id'],\n",
    "            'situation_idx': situation['situation_idx'],\n",
    "            'situation_latent': pi_latent.squeeze().detach().cpu().numpy()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'demo_id': 'Player757-db89409b658a-20220705-000459',\n",
       " 'situation_idx': 128,\n",
       " 'situation_latent': array([ 0.7239679, -0.6623365,  1.3408374, ...,  1.3709091, -0.6623365,\n",
       "        -0.6623365], dtype=float32)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "situation_latents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e760dec67e3d46708d79269dedcd0c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo0 = memory.vpt.encode_trajectory(demonstrations[0]['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(demo0[128], situation_latents[0]['situation_latent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(demo0[130], mem_situations[1]['situation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(mem_situations[0]['situation'], situation_latents[0]['situation_latent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_loader.save_situations(situation_latents, \"data/situation_latents.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1024)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "situation_latents_array = np.array([x['situation_latent'] for x in situation_latents])\n",
    "situation_latents_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "\n",
    "    def create_index(self, situation_latents_array):\n",
    "        self.index = faiss.IndexFlatL2(1024)\n",
    "        self.index.add(situation_latents_array)\n",
    "\n",
    "    def save_index(self, save_path):\n",
    "        faiss.write_index(self.index, save_path)\n",
    "    \n",
    "    def load_index(self, save_path):\n",
    "        self.index = faiss.read_index(save_path)\n",
    "\n",
    "    def search(self, query, k=4):\n",
    "        distances, nearest_indices = self.index.search(query.reshape(1, 1024), k)\n",
    "        return distances[0], nearest_indices[0]\n",
    "    \n",
    "    def embeddings(self):\n",
    "        pass\n",
    "\n",
    "    def create_situations(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_index(\"data/memory.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_query = situation_latents_array[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.    ,  741.8623, 1323.79  , 1340.2336], dtype=float32),\n",
       " array([9, 8, 1, 0]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.search(situation_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.26986554e-01 1.43079794e-01 2.47547281e-02 4.28290082e-03\n",
      " 7.40999430e-04 1.28202865e-04 2.21808194e-05 3.83758000e-06\n",
      " 6.63952941e-07 1.14872786e-07 1.98745366e-08 3.43856207e-09\n",
      " 5.94917472e-10 1.02928722e-10 1.78080529e-11 3.08103260e-12\n",
      " 5.33060067e-13 9.22265591e-14 1.59564348e-14 2.76067777e-15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f79d75dfe50>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy library\n",
    "import numpy as np\n",
    "\n",
    "# define the exponential decay function\n",
    "def exp_decay(t, A, tau, C):\n",
    "    return A * np.exp(-t / tau) + C\n",
    "\n",
    "# define the time array (you can change this as you like)\n",
    "t = np.linspace(0, 5, 20)\n",
    "\n",
    "# define the fixed parameters for the exponential decay function\n",
    "A = 1\n",
    "tau = 0.15\n",
    "C = 0\n",
    "\n",
    "# generate an array of 64 numbers that follow the exponential decay curve\n",
    "array = exp_decay(t, A, tau, C)\n",
    "\n",
    "# divide the array by its sum to make it add up to 1\n",
    "array = np.array(array / np.sum(array))\n",
    "\n",
    "# print the array\n",
    "print(array)\n",
    "\n",
    "# plot the array\n",
    "plt.plot(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
