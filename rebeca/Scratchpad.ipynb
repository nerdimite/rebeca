{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import gym\n",
    "import minerl\n",
    "import torch\n",
    "import torch as th\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import faiss\n",
    "\n",
    "from openai_vpt.agent import PI_HEAD_KWARGS, MineRLAgent\n",
    "from openai_vpt.lib.policy import MinecraftPolicy\n",
    "from data_loader import DataLoader\n",
    "from openai_vpt.lib.tree_util import tree_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob(\"data/MakeWaterfallTrain/*.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USING_FULL_DATASET = False\n",
    "\n",
    "EPOCHS = 1 if USING_FULL_DATASET else 1\n",
    "# Needs to be <= number of videos\n",
    "BATCH_SIZE = 64 if USING_FULL_DATASET else 16\n",
    "# Ideally more than batch size to create\n",
    "# variation in datasets (otherwise, you will\n",
    "# get a bunch of consecutive samples)\n",
    "# Decrease this (and batch_size) if you run out of memory\n",
    "N_WORKERS = 100 if USING_FULL_DATASET else 16\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "LOSS_REPORT_RATE = 100\n",
    "\n",
    "# Tuned with bit of trial and error\n",
    "LEARNING_RATE = 0.000181\n",
    "# OpenAI VPT BC weight decay\n",
    "# WEIGHT_DECAY = 0.039428\n",
    "WEIGHT_DECAY = 0.0\n",
    "# KL loss to the original model was not used in OpenAI VPT\n",
    "KL_LOSS_WEIGHT = 1.0\n",
    "MAX_GRAD_NORM = 5.0\n",
    "\n",
    "MAX_BATCHES = 2000 if USING_FULL_DATASET else int(1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_parameters(path_to_model_file):\n",
    "    agent_parameters = pickle.load(open(path_to_model_file, \"rb\"))\n",
    "    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n",
    "    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n",
    "    return policy_kwargs, pi_head_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_model = \"data/VPT-models/foundation-model-1x.model\"\n",
    "in_weights = \"data/VPT-models/foundation-model-1x.weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs, pi_head_kwargs = load_model_parameters(in_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs['recurrence_type'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpt_cnn = MinecraftPolicy(**policy_kwargs, single_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpt_cnn.recurrent_layer = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinecraftPolicy(\n",
       "  (img_preprocess): ImgPreprocessing()\n",
       "  (img_process): ImgObsProcess(\n",
       "    (cnn): ImpalaCNN(\n",
       "      (stacks): ModuleList(\n",
       "        (0): CnnDownStack(\n",
       "          (firstconv): FanInInitReLULayer(\n",
       "            (layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (n): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (blocks): ModuleList(\n",
       "            (0): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CnnDownStack(\n",
       "          (firstconv): FanInInitReLULayer(\n",
       "            (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (layer): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (n): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (blocks): ModuleList(\n",
       "            (0): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CnnDownStack(\n",
       "          (firstconv): FanInInitReLULayer(\n",
       "            (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (n): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (blocks): ModuleList(\n",
       "            (0): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (1): CnnBasicBlock(\n",
       "              (conv0): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "              (conv1): FanInInitReLULayer(\n",
       "                (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "                (layer): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dense): FanInInitReLULayer(\n",
       "        (norm): LayerNorm((32768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer): Linear(in_features=32768, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): FanInInitReLULayer(\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer): Linear(in_features=256, out_features=1024, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (recurrent_layer): Identity()\n",
       "  (lastlayer): FanInInitReLULayer(\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  )\n",
       "  (final_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpt_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MineRLAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m agent_policy_kwargs, agent_pi_head_kwargs \u001b[39m=\u001b[39m load_model_parameters(in_model)\n\u001b[0;32m----> 3\u001b[0m agent \u001b[39m=\u001b[39m MineRLAgent(device\u001b[39m=\u001b[39mDEVICE, policy_kwargs\u001b[39m=\u001b[39magent_policy_kwargs, pi_head_kwargs\u001b[39m=\u001b[39magent_pi_head_kwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MineRLAgent' is not defined"
     ]
    }
   ],
   "source": [
    "agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(in_model)\n",
    "\n",
    "agent = MineRLAgent(device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictActionHead(\n",
       "  (camera): CategoricalActionHead(\n",
       "    (linear_layer): Linear(in_features=1024, out_features=121, bias=True)\n",
       "  )\n",
       "  (buttons): CategoricalActionHead(\n",
       "    (linear_layer): Linear(in_features=1024, out_features=8641, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.pi_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_vpt.lib.action_mapping import CameraHierarchicalMapping\n",
    "from openai_vpt.lib.actions import ActionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_TRANSFORMER_KWARGS = dict(\n",
    "    camera_binsize=2,\n",
    "    camera_maxval=10,\n",
    "    camera_mu=10,\n",
    "    camera_quantization_scheme=\"mu_law\",\n",
    ")\n",
    "action_mapper = CameraHierarchicalMapping(n_camera_bins=11)\n",
    "action_transformer = ActionTransformer(**ACTION_TRANSFORMER_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_action_to_agent(minerl_action_transformed, to_torch=False, check_if_null=False):\n",
    "    \"\"\"\n",
    "    Turn action from MineRL to model's action.\n",
    "\n",
    "    Note that this will add batch dimensions to the action.\n",
    "    Returns numpy arrays, unless `to_torch` is True, in which case it returns torch tensors.\n",
    "\n",
    "    If `check_if_null` is True, check if the action is null (no action) after the initial\n",
    "    transformation. This matches the behaviour done in OpenAI's VPT work.\n",
    "    If action is null, return \"None\" instead\n",
    "    \"\"\"\n",
    "    minerl_action = action_transformer.env2policy(minerl_action_transformed)\n",
    "    if check_if_null:\n",
    "        if np.all(minerl_action[\"buttons\"] == 0) and np.all(minerl_action[\"camera\"] == action_transformer.camera_zero_bin):\n",
    "            return None\n",
    "\n",
    "    # Add batch dims if not existant\n",
    "    if minerl_action[\"camera\"].ndim == 1:\n",
    "        minerl_action = {k: v[None] for k, v in minerl_action.items()}\n",
    "    action = action_mapper.from_factored(minerl_action)\n",
    "    if to_torch:\n",
    "        action = {k: th.from_numpy(v).to(DEVICE) for k, v in action.items()}\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"data/MakeWaterfall/Player571-f153ac423f61-20220707-110239.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames(video_path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(jsonl_path):\n",
    "    with open(jsonl_path) as f:\n",
    "        return [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = load_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_jsonl(\"data/MakeWaterfall/Player571-f153ac423f61-20220707-110239.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(frames):\n",
    "    for frame in frames:\n",
    "        cv2.imshow(\"image\", frame)\n",
    "        cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = glob.glob(os.path.join('data/MakeWaterfall/', \"*.mp4\"))\n",
    "unique_ids = list(set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_first = th.from_numpy(np.array((False,))).to(DEVICE).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPTModel():\n",
    "    '''VPT Model for Embedding Situations and Single Observations'''\n",
    "\n",
    "    def __init__(self, model_path, weights_path=None, freeze=True, device='auto'):\n",
    "\n",
    "        agent_policy_kwargs = self.load_model_parameters(model_path)\n",
    "        self.policy = MinecraftPolicy(\n",
    "            **agent_policy_kwargs, single_output=True)\n",
    "\n",
    "        if device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.policy.to(self.device)\n",
    "\n",
    "        if weights_path is not None:\n",
    "            self.policy.load_state_dict(torch.load(\n",
    "                weights_path, map_location=self.device))\n",
    "\n",
    "        if freeze:\n",
    "            for param in self.policy.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dummy_first = torch.from_numpy(np.array((False,))).to(self.device).unsqueeze(1)\n",
    "\n",
    "    def load_model_parameters(self, model_path):\n",
    "        '''Load model parameters from model_path'''\n",
    "\n",
    "        with open(model_path, 'rb') as f:\n",
    "            agent_parameters = pickle.load(f)\n",
    "            policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n",
    "\n",
    "            return policy_kwargs\n",
    "\n",
    "    def preprocess_obs(self, obs_frame):\n",
    "        '''Turn observation from MineRL environment into model's observation'''\n",
    "        policy_input = cv2.resize(\n",
    "            obs_frame, (128, 128), interpolation=cv2.INTER_LINEAR)[None]\n",
    "        policy_input = {\"img\": torch.from_numpy(policy_input).to(self.device)}\n",
    "        return policy_input\n",
    "    \n",
    "    def encode(self, obs, state_in):\n",
    "        '''Encode observation into latent space'''\n",
    "\n",
    "        obs = self.preprocess_obs(obs)\n",
    "        obs = tree_map(lambda x: x.unsqueeze(1), obs)\n",
    "        latent_vec, state_out = self.policy(obs, state_in, context={\"first\": self.dummy_first})\n",
    "        \n",
    "        return latent_vec, state_out\n",
    "    \n",
    "    def encode_trajectory(self, trajectory):\n",
    "        '''Encode expert trajectory frames into a latent vector with state history'''\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            initial_state = self.policy.initial_state(1)\n",
    "            hidden_state = initial_state\n",
    "            latent_vectors = []\n",
    "\n",
    "            for obs in tqdm(trajectory):\n",
    "                latent, state_out = self.encode(obs, hidden_state)\n",
    "                hidden_state = state_out\n",
    "                latent_vectors.append(latent.squeeze().detach().cpu().numpy())\n",
    "\n",
    "            return latent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SituationsLoader():\n",
    "    '''Load the data from the MakeWaterfall dataset and create situations'''\n",
    "    def __init__(self, data_dir='data/MakeWaterfall/'):\n",
    "        unique_ids = glob.glob(os.path.join(data_dir, \"*.mp4\"))\n",
    "        unique_ids = list(set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))[:1]\n",
    "\n",
    "        self.demonstration_tuples = []\n",
    "        for unique_id in unique_ids:\n",
    "            video_path = os.path.abspath(os.path.join(data_dir, unique_id + \".mp4\"))\n",
    "            json_path = os.path.abspath(os.path.join(data_dir, unique_id + \".jsonl\"))\n",
    "            self.demonstration_tuples.append((unique_id, video_path, json_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.demonstration_tuples)\n",
    "    \n",
    "    def _load_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    def _load_jsonl(self, jsonl_path):\n",
    "        with open(jsonl_path) as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "        \n",
    "    def build_situations(self, window_size=128, stride=2):\n",
    "        \n",
    "        situations = []\n",
    "        for unique_id, video_path, json_path in tqdm(self.demonstration_tuples):\n",
    "            video = self._load_video(video_path)\n",
    "            jsonl = self._load_jsonl(json_path)\n",
    "            \n",
    "            for i in range(window_size, len(video) - window_size, stride):\n",
    "                situation = {}\n",
    "                situation['demo_id'] = unique_id\n",
    "                situation['situation_idx'] = i\n",
    "                situation['situation_obs'] = video[i-window_size:i+1] # 128 context + 1 current\n",
    "                situations.append(situation)\n",
    "                \n",
    "        return situations\n",
    "    \n",
    "    def load_demonstrations(self):\n",
    "        demonstrations = []\n",
    "        for unique_id, video_path, json_path in tqdm(self.demonstration_tuples):\n",
    "            video = self._load_video(video_path)\n",
    "            jsonl = self._load_jsonl(json_path)\n",
    "            demonstrations.append({\n",
    "                'demo_id': unique_id,\n",
    "                'video': video,\n",
    "                'jsonl': jsonl\n",
    "\n",
    "            })\n",
    "        return demonstrations\n",
    "    \n",
    "    def save_situations(self, situations, save_path):\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(situations, f)\n",
    "\n",
    "    def load_situations(self, save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SituationsLoader2():\n",
    "    '''Complete External Memory System for REBECA'''\n",
    "\n",
    "    def __init__(self, model_path, weights_path=None, freeze=True, device='auto'):\n",
    "        self.vpt = VPTModel(model_path, weights_path, freeze, device)\n",
    "\n",
    "    def load_expert_data(self, data_dir='data/MakeWaterfall/'):\n",
    "        '''Load expert demonstrations from data_dir'''\n",
    "\n",
    "        unique_ids = glob.glob(os.path.join(data_dir, \"*.mp4\"))\n",
    "        unique_ids = list(\n",
    "            set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))[:1]\n",
    "\n",
    "        self.demonstration_tuples = []\n",
    "        for unique_id in unique_ids:\n",
    "            video_path = os.path.abspath(\n",
    "                os.path.join(data_dir, unique_id + \".mp4\"))\n",
    "            json_path = os.path.abspath(\n",
    "                os.path.join(data_dir, unique_id + \".jsonl\"))\n",
    "            self.demonstration_tuples.append(\n",
    "                (unique_id, video_path, json_path))\n",
    "            \n",
    "    def load_demonstrations(self):\n",
    "        demonstrations = []\n",
    "        for unique_id, video_path, json_path in tqdm(self.demonstration_tuples):\n",
    "            video = self._load_video(video_path)\n",
    "            jsonl = self._load_jsonl(json_path)\n",
    "            demonstrations.append({\n",
    "                'demo_id': unique_id,\n",
    "                'video': video,\n",
    "                'jsonl': jsonl\n",
    "\n",
    "            })\n",
    "        return demonstrations\n",
    "    \n",
    "    def encode_demonstrations(self, demonstrations):\n",
    "        encoded_demos = []\n",
    "        for demo in tqdm(demonstrations):\n",
    "            encoded_demo = self.vpt.encode_trajectory(demo['video'])\n",
    "            encoded_demos.append({\n",
    "                'demo_id': demo['demo_id'],\n",
    "                'encoded_demo': encoded_demo\n",
    "            })\n",
    "        return encoded_demos\n",
    "    \n",
    "    def create_situations(self, encoded_demos, window_size=128, stride=2):\n",
    "        situations = []\n",
    "        for demo in tqdm(encoded_demos):\n",
    "            for i in range(window_size, len(demo['encoded_demo']) - window_size, stride):\n",
    "                situations.append({\n",
    "                    'demo_id': demo['demo_id'],\n",
    "                    'situation_idx': i,\n",
    "                    'situation': demo['encoded_demo'][i]\n",
    "                })\n",
    "        return situations\n",
    "\n",
    "    def _load_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (128, 128),\n",
    "                               interpolation=cv2.INTER_LINEAR)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def _load_jsonl(self, jsonl_path):\n",
    "        with open(jsonl_path) as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "\n",
    "    def save_situations(self, situations, save_path):\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(situations, f)\n",
    "\n",
    "    def load_situations(self, save_path):\n",
    "        with open(save_path, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_loader = SituationsLoader()\n",
    "memory = Memory(in_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_expert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff01f5f78cd4855858baff3dbac3f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demonstrations = memory.load_demonstrations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f57ab1a97174397b9ed0ed6f165870f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752f189032c64c0e95593fec31d1aa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255defc0c8554b3fab0ceaf0d803c27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_demos = memory.encode_demonstrations(demonstrations)\n",
    "mem_situations = memory.create_situations(encoded_demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mem_situations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7332349bc4844275b744fdd92ebb910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "situations = situation_loader.build_situations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 49923)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(situations), len(situations) * 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf6f8b9217b4d8e9b3f1c671fc26959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demonstrations = situation_loader.load_demonstrations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(demo['video']) for demo in demonstrations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50e5e0a83be4c81ac7b5a33391f1910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "situation_latents = []\n",
    "\n",
    "with th.inference_mode():\n",
    "    for situation in tqdm(situations[:3]):\n",
    "        situation_obs = situation['situation_obs']\n",
    "\n",
    "        initial_state = memory.vpt.policy.initial_state(1)\n",
    "        states = [initial_state]\n",
    "\n",
    "        for obs in situation_obs:\n",
    "            obs = memory.vpt.preprocess_obs(obs)\n",
    "            obs = tree_map(lambda x: x.unsqueeze(1), obs)\n",
    "            pi_latent, state_out = memory.vpt.policy(obs, states[-1], context={\"first\": dummy_first})\n",
    "            states.append(state_out)\n",
    "        \n",
    "        situation_latents.append({\n",
    "            'demo_id': situation['demo_id'],\n",
    "            'situation_idx': situation['situation_idx'],\n",
    "            'situation_latent': pi_latent.squeeze().detach().cpu().numpy()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'demo_id': 'Player757-db89409b658a-20220705-000459',\n",
       " 'situation_idx': 128,\n",
       " 'situation_latent': array([ 0.7239679, -0.6623365,  1.3408374, ...,  1.3709091, -0.6623365,\n",
       "        -0.6623365], dtype=float32)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "situation_latents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e760dec67e3d46708d79269dedcd0c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo0 = memory.vpt.encode_trajectory(demonstrations[0]['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(demo0[128], situation_latents[0]['situation_latent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(demo0[130], mem_situations[1]['situation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(mem_situations[0]['situation'], situation_latents[0]['situation_latent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_loader.save_situations(situation_latents, \"data/situation_latents.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1024)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "situation_latents_array = np.array([x['situation_latent'] for x in situation_latents])\n",
    "situation_latents_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "\n",
    "    def create_index(self, situation_latents_array):\n",
    "        self.index = faiss.IndexFlatL2(1024)\n",
    "        self.index.add(situation_latents_array)\n",
    "\n",
    "    def save_index(self, save_path):\n",
    "        faiss.write_index(self.index, save_path)\n",
    "    \n",
    "    def load_index(self, save_path):\n",
    "        self.index = faiss.read_index(save_path)\n",
    "\n",
    "    def search(self, query, k=4):\n",
    "        distances, nearest_indices = self.index.search(query.reshape(1, 1024), k)\n",
    "        return distances[0], nearest_indices[0]\n",
    "    \n",
    "    def embeddings(self):\n",
    "        pass\n",
    "\n",
    "    def create_situations(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_index(\"data/memory.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_query = situation_latents_array[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.    ,  741.8623, 1323.79  , 1340.2336], dtype=float32),\n",
       " array([9, 8, 1, 0]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.search(situation_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
