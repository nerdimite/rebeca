{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import VPTEncoder, Controller\n",
    "from memory import SituationLoader, Memory\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from openai_vpt.lib.action_mapping import CameraHierarchicalMapping\n",
    "from openai_vpt.lib.actions import ActionTransformer\n",
    "from action_utils import ActionProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_model = \"data/VPT-models/foundation-model-1x.model\"\n",
    "in_weights = \"data/VPT-models/foundation-model-1x-net.weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(in_weights, map_location=\"cpu\")\n",
    "# # keep weights only for the net part\n",
    "# state = {k: v for k, v in state.items() if k.startswith(\"net.\")}\n",
    "# # remove the \"net.\" prefix\n",
    "# state = {k[4:]: v for k, v in state.items()}\n",
    "# torch.save(state, \"data/VPT-models/foundation-model-1x-net.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpt = VPTEncoder(in_model, in_weights)\n",
    "vpt.eval()\n",
    "expert_dataloader = SituationLoader(vpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd42f3cbe5f4ac98d2cb101fc078400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading expert demonstrations:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demonstrations = expert_dataloader.load_demonstrations(num_demos=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c494617d9414c2ebd825cd67dda67fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding expert demonstrations:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa422eed1456439383d74721fb1ae7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding Trajectory:   0%|          | 0/1662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02696a2dd19741c69480f772083e7837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding Trajectory:   0%|          | 0/1410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_demos = expert_dataloader.encode_demonstrations(demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf86b239d64c4df99b56ad2e5a802a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating situations:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "situations = expert_dataloader.create_situations(encoded_demos, stride=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "#   VPT Encoder (Frozen)\n",
    "# \tMemory\n",
    "# Rebeca Policy\n",
    "# \tRetriever\n",
    "# \tVPT Backbone (Trainable)\n",
    "# \tController\n",
    "# Forward\n",
    "# \tRetrieve Situations\n",
    "# \tobs = VPT Backbone (obs)\n",
    "# \tpreprocess retrieved situations\n",
    "# \tkey, cam = Controller(obs, situations, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever():\n",
    "    def __init__(self, encoder_model, encoder_weights, memory_path):\n",
    "        self.vpt = VPTEncoder(encoder_model, encoder_weights)\n",
    "        self.vpt.eval()\n",
    "        self.memory = Memory()\n",
    "        self.memory.load_index(memory_path)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def encode_query(self, query_obs):\n",
    "        query_obs_vec, state_out = self.vpt(query_obs, self.hidden_state)\n",
    "        self.hidden_state = state_out\n",
    "        query_obs_vec = query_obs_vec.squeeze().cpu().numpy()\n",
    "        return query_obs_vec\n",
    "\n",
    "    def retrieve(self, query_obs, k=2, encode_obs=True):\n",
    "        if encode_obs:\n",
    "            query_obs = self.encode_query(query_obs)\n",
    "        results = self.memory.search(query_obs, k=k)\n",
    "\n",
    "        if results[0]['distance'] == 0: # to prevent returning the same situation and overfitting\n",
    "            print(\"Same situation found\")\n",
    "            return results[1], query_obs\n",
    "        else:\n",
    "            return results[0], query_obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.hidden_state = self.vpt.policy.initial_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(in_model, in_weights, \"data/memory.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_video(video_path):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demonstration(demo_id, root_dir=\"data/MakeWaterfall\"):\n",
    "    video_path = f\"{root_dir}/{demo_id}.mp4\"\n",
    "    frames = _load_video(video_path)\n",
    "    return frames\n",
    "\n",
    "def load_situation(situation_id, demo_frames):\n",
    "    situation = demo_frames[situation_id]\n",
    "    return situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in demonstrations[0]['video'][150:152]:\n",
    "    result, query_obs_vec = retriever.retrieve(obs)\n",
    "    print(result['distance'])\n",
    "    plt.imshow(obs[..., ::-1])\n",
    "    plt.show()\n",
    "    res_demo = load_demonstration(result['demo_id'])\n",
    "    res_situation = load_situation(result['sit_frame_idx'], res_demo)\n",
    "    plt.imshow(res_situation[..., ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(a, b):\n",
    "    return euclidean(a, b) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261.08333681715885"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_distance(query_obs_vec, retriever.memory.index.reconstruct(38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260.82249999999993"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16.15**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261.08337"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.memory.index.reconstruct(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['idx', 'demo_id', 'sit_frame_idx', 'distance', 'actions'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.8993337 , -0.21491544, -0.32291505, ..., -0.08695894,\n",
       "       -0.03951202, -0.15778641], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.memory.index.reconstruct(int(result['idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REBECA(nn.Module):\n",
    "    def __init__(self, encoder_model, encoder_weights, memory_path, device='auto'):\n",
    "        super().__init__()\n",
    "\n",
    "        if device == 'auto':\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.retriever = Retriever(encoder_model, encoder_weights, memory_path)\n",
    "        \n",
    "        # Unfreeze final layers\n",
    "        self.vpt = VPTEncoder(encoder_model, encoder_weights, freeze=True)\n",
    "        self.trainable_parameters = []\n",
    "        for param in self.vpt.policy.lastlayer.parameters():\n",
    "            param.requires_grad = True\n",
    "            self.trainable_parameters.append(param)\n",
    "\n",
    "        self.controller = Controller()\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # TODO: retrieve situations only if the agent diverges from the previous situation\n",
    "        if self.current_situation is None:\n",
    "            result, query_obs_vec = self.retriever.retrieve(obs)\n",
    "            self.current_situation = result\n",
    "        else:\n",
    "            query_obs_vec = self.retriever.encode_query(obs)\n",
    "            if l2_distance(query_obs_vec, retriever.memory.index.reconstruct(self.current_situation['idx'])) > 300:\n",
    "                result, query_obs_vec = self.retriever.retrieve(query_obs_vec, encode_obs=False)\n",
    "                self.current_situation = result\n",
    "\n",
    "        situation = torch.Tensor(self.retriever.memory.index.reconstruct(result['idx'])).reshape(1, 1, -1)\n",
    "        obs_vector, self.hidden_state = self.vpt(obs, self.hidden_state)\n",
    "\n",
    "        retrieved_actions = {\n",
    "            \"camera\": self._one_hot_encode(result['actions']['camera'], 121).to(self.device),\n",
    "            \"keyboard\": self._one_hot_encode(result['actions']['buttons'], 8641).to(self.device)\n",
    "        }\n",
    "        \n",
    "        action = self.controller(\n",
    "            obs_vector.to(self.device), \n",
    "            situation.to(self.device), \n",
    "            retrieved_actions\n",
    "        )\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reset(self):\n",
    "        self.hidden_state = self.vpt.policy.initial_state(1)\n",
    "        self.current_situation = None\n",
    "        self.retriever.reset()\n",
    "\n",
    "    def _one_hot_encode(self, actions: list, num_classes: int, add_batch_dim: bool = True):\n",
    "        '''One-hot encodes the actions'''\n",
    "        actions = torch.tensor(actions)\n",
    "        if add_batch_dim:\n",
    "            actions = actions.unsqueeze(0)\n",
    "        return torch.nn.functional.one_hot(actions, num_classes=num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebeca = REBECA(in_model, in_weights, \"data/memory.json\", device='cuda')\n",
    "rebeca.to(rebeca.device)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebeca.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in demonstrations[0]['video'][0:2]:\n",
    "    actions = rebeca(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0514, -0.1729,  0.2063,  ...,  0.0101, -0.1178, -0.0225]],\n",
       "       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
