{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import VPTEncoder, Controller\n",
    "from memory import SituationLoader, Memory\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from openai_vpt.lib.action_mapping import CameraHierarchicalMapping\n",
    "from openai_vpt.lib.actions import ActionTransformer\n",
    "from action_utils import ActionProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_model = \"data/VPT-models/foundation-model-1x.model\"\n",
    "in_weights = \"data/VPT-models/foundation-model-1x.weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vpt = VPTEncoder(in_model)\n",
    "vpt.eval()\n",
    "expert_dataloader = SituationLoader(vpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a740dad8ee70465f91d78e4cc8d7f4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading expert demonstrations:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demonstrations = expert_dataloader.load_demonstrations(num_demos=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28f41972ffa43d7afc3caf9991d6968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding expert demonstrations:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371b9c797ac14f0ca54c1e9204ac033d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding Trajectory:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372726624516441c9df07b5d3b721765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding Trajectory:   0%|          | 0/1255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_demos = expert_dataloader.encode_demonstrations(demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "situations = expert_dataloader.create_situations(encoded_demos, stride=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_demos[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "situations[0]['actions']['buttons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "situations[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.create_index(situations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_index(save_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_index('data/memory.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = situations[0][\"situation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = memory.search(query, k=5)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(demonstrations[0][\"video\"][res[0][\"sit_frame_idx\"]][..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(demonstrations[1][\"video\"][res[1][\"sit_frame_idx\"]][..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(list(filter(lambda x: x['demo_id'] == 'Player648-f6ef373a67fd-20220706-200136', demonstrations))[0]['video'][res[3]['situation_idx']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some dummy inputs\n",
    "batch_size = 16\n",
    "seq_len = 10\n",
    "embed_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "query = torch.randn(1, batch_size, embed_dim) # The query sequence\n",
    "key = torch.randn(seq_len, batch_size, embed_dim) # The key sequence\n",
    "value = torch.randn(seq_len, batch_size, embed_dim) # The value sequence\n",
    "\n",
    "# Create a multihead attention layer\n",
    "attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "\n",
    "# Apply cross attention\n",
    "output, weights = attn(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "d_model = 1024 # dimension of the input and output vectors\n",
    "d_k = 1024 # dimension of the query and key vectors\n",
    "d_v = 1024 # dimension of the value vectors\n",
    "n_heads = 4 # number of attention heads\n",
    "assert d_model % n_heads == 0 # make sure d_model is divisible by n_heads\n",
    "\n",
    "# Define linear layers for projection\n",
    "Wq_obs = nn.Linear(d_model, d_k) # project observation embedding to query vector\n",
    "Wk_sit = nn.Linear(d_model, d_k) # project situation embedding to key vector\n",
    "Wv_sit = nn.Linear(d_model, d_v) # project situation embedding to value vector\n",
    "Wk_key = nn.Linear(8641, d_k) # project keyboard action one-hot vector to key vector\n",
    "Wv_key = nn.Linear(8641, d_v) # project keyboard action one-hot vector to value vector\n",
    "Wk_cam = nn.Linear(121, d_k) # project camera action one-hot vector to key vector\n",
    "Wv_cam = nn.Linear(121, d_v) # project camera action one-hot vector to value vector\n",
    "\n",
    "# Define multi-head attention layer\n",
    "MHA = nn.MultiheadAttention(d_model, n_heads)\n",
    "\n",
    "# Define output layer for concatenation or addition\n",
    "Wo = nn.Linear(d_v, d_model) # project output vector to original dimension\n",
    "\n",
    "# Define input tensors\n",
    "obs = torch.randn(1, 1, d_model) # observation embedding tensor of shape [1, 1, 1024]\n",
    "sit = torch.randn(1, 1, d_model) # situation embedding tensor of shape [1, 1, 1024]\n",
    "key = torch.randn(1, 128, 8641) # keyboard action one-hot tensor of shape [1, 128, 8641]\n",
    "cam = torch.randn(1, 128, 121) # camera action one-hot tensor of shape [1, 128, 121]\n",
    "\n",
    "# Project input tensors to query, key and value vectors\n",
    "q_obs = Wq_obs(obs) # query vector tensor of shape [1 ,1 ,1024]\n",
    "k_sit = Wk_sit(sit) # key vector tensor of shape [1 ,1 ,1024]\n",
    "v_sit = Wv_sit(sit) # value vector tensor of shape [1 ,1 ,1024]\n",
    "k_key = Wk_key(key) # key vector tensor of shape [1 ,128 ,1024]\n",
    "v_key = Wv_key(key) # value vector tensor of shape [1 ,128 ,1024]\n",
    "k_cam = Wk_cam(cam) # key vector tensor of shape [1 ,128 ,1024]\n",
    "v_cam = Wv_cam(cam) # value vector tensor of shape [1 ,128 ,1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the key and value vectors along the second dimension\n",
    "k_all = torch.cat([k_sit, k_key, k_cam], dim=1) # key vector tensor of shape [1 ,257 ,1024]\n",
    "v_all = torch.cat([v_sit, v_key, v_cam], dim=1) # value vector tensor of shape [1 ,257 ,1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multi-head attention on the query and key-value pairs\n",
    "out_obs, _ = MHA(q_obs, k_all.transpose(0 ,1), v_all.transpose(0 ,1)) \n",
    "# output vector tensor of shape [1 ,1 ,1024]\n",
    "\n",
    "# Optionally, you can concatenate or add the output vector with the original query vector\n",
    "out_obs = torch.cat([out_obs.transpose(0 ,1), q_obs], dim=1)\n",
    "# output vector tensor of shape [1 ,2 ,1024] after concatenation\n",
    "# out_obs = out_obs + q_obs \n",
    "# output vector tensor of shape [1 ,2 ,1024] after addition\n",
    "\n",
    "# Apply output layer on the output vector\n",
    "# out_obs = Wo(out_obs) \n",
    "# output vector tensor of shape [1 ,2 ,1024] after projection\n",
    "\n",
    "out_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_observation = torch.Tensor(situations[0][\"situation\"]).reshape(1, 1, -1)\n",
    "_situation = torch.Tensor(situations[0][\"situation\"]).reshape(1, 1, -1)\n",
    "_actions = situations[10][\"actions\"]\n",
    "_actions = {\n",
    "    \"camera\": torch.Tensor(_actions[\"camera\"]).unsqueeze(0),\n",
    "    \"keyboard\": torch.Tensor(_actions[\"buttons\"]).unsqueeze(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_key, out_cam = controller(_observation, _situation, _actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
