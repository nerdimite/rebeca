{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from rebeca import REBECA\n",
    "from data_loader import DataLoader\n",
    "from openai_vpt.lib.tree_util import tree_map\n",
    "\n",
    "# Originally this code was designed for a small dataset of ~20 demonstrations per task.\n",
    "# The settings might not be the best for the full BASALT dataset (thousands of demonstrations).\n",
    "# Use this flag to switch between the two settings\n",
    "USING_FULL_DATASET = False\n",
    "\n",
    "EPOCHS = 1 if USING_FULL_DATASET else 2\n",
    "# Needs to be <= number of videos\n",
    "BATCH_SIZE = 64 if USING_FULL_DATASET else 1\n",
    "# Ideally more than batch size to create\n",
    "# variation in datasets (otherwise, you will\n",
    "# get a bunch of consecutive samples)\n",
    "# Decrease this (and batch_size) if you run out of memory\n",
    "N_WORKERS = 100 if USING_FULL_DATASET else 1\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "LOSS_REPORT_RATE = 10\n",
    "\n",
    "# Tuned with bit of trial and error\n",
    "LEARNING_RATE = 0.000181\n",
    "# OpenAI VPT BC weight decay\n",
    "# WEIGHT_DECAY = 0.039428\n",
    "WEIGHT_DECAY = 0.0\n",
    "# KL loss to the original model was not used in OpenAI VPT\n",
    "KL_LOSS_WEIGHT = 1.0\n",
    "MAX_GRAD_NORM = 5.0\n",
    "\n",
    "MAX_BATCHES = 2000 if USING_FULL_DATASET else int(1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_model = \"data/VPT-models/foundation-model-1x.model\"\n",
    "in_weights = \"data/VPT-models/foundation-model-1x.weights\"\n",
    "cnn_weights = \"data/VPT-models/foundation-model-1x-cnn.weights\"\n",
    "trf_weights = \"data/VPT-models/foundation-model-1x-trf.weights\"\n",
    "data_dir = \"data/MakeWaterfallTrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = REBECA(in_model, cnn_weights, trf_weights, \"data/memory_cnn.json\", device=DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in agent.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze controller layers\n",
    "trainable_parameters = []\n",
    "for name, param in agent.named_parameters():\n",
    "    if not name.startswith(\"controller.vpt_transformers\"):\n",
    "        param.requires_grad = True\n",
    "        trainable_parameters.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters taken from the OpenAI VPT paper\n",
    "optimizer = torch.optim.Adam(\n",
    "    trainable_parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset_dir=data_dir,\n",
    "    n_workers=N_WORKERS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=EPOCHS,\n",
    "    num_demonstrations=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     episode_hidden_states[episode_id] \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mcontroller\u001b[39m.\u001b[39minitial_state(\u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m agent_state \u001b[39m=\u001b[39m episode_hidden_states[episode_id]\n\u001b[0;32m---> 30\u001b[0m pred_action, new_agent_state \u001b[39m=\u001b[39m agent(image, agent_state)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Make sure we do not try to backprop through sequence\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# (fails with current accumulation)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m new_agent_state \u001b[39m=\u001b[39m tree_map(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mdetach(), new_agent_state)\n",
      "File \u001b[0;32m/opt/conda/envs/minerl/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/rebeca/rebeca/rebeca.py:172\u001b[0m, in \u001b[0;36mREBECA.forward\u001b[0;34m(self, obs, state_in)\u001b[0m\n\u001b[1;32m    169\u001b[0m obs_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvpt_cnn(obs)\n\u001b[1;32m    171\u001b[0m \u001b[39m# retrieve situation from memory\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m situation, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretriever\u001b[39m.\u001b[39;49mretrieve(obs_feats\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m), k\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, encode_obs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    174\u001b[0m \u001b[39m# process retrieved situations\u001b[39;00m\n\u001b[1;32m    175\u001b[0m situation_embed, situation_actions, next_action \u001b[39m=\u001b[39m preprocess_situation(situation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/workspace/rebeca/rebeca/rebeca.py:137\u001b[0m, in \u001b[0;36mRetriever.retrieve\u001b[0;34m(self, query_obs, k, encode_obs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m encode_obs:\n\u001b[1;32m    136\u001b[0m     query_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_query(query_obs)\n\u001b[0;32m--> 137\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msearch(query_obs, k\u001b[39m=\u001b[39;49mk)\n\u001b[1;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m results[\u001b[39m0\u001b[39m], query_obs\n",
      "File \u001b[0;32m~/workspace/rebeca/rebeca/memory.py:194\u001b[0m, in \u001b[0;36mMemory.search\u001b[0;34m(self, query, k)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch\u001b[39m(\u001b[39mself\u001b[39m, query, k\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m):\n\u001b[0;32m--> 194\u001b[0m     distances, nearest_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49msearch(query\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m1024\u001b[39;49m), k)\n\u001b[1;32m    195\u001b[0m     result \u001b[39m=\u001b[39m []\n\u001b[1;32m    196\u001b[0m     \u001b[39mfor\u001b[39;00m i, idx \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(nearest_indices[\u001b[39m0\u001b[39m]):\n",
      "File \u001b[0;32m/opt/conda/envs/minerl/lib/python3.8/site-packages/faiss/class_wrappers.py:328\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_search\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Find the k nearest neighbors of the set of vectors x in the index.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39m    When not enough results are found, the label is set to -1\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m n, d \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 328\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mascontiguousarray(x, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    329\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[1;32m    331\u001b[0m \u001b[39massert\u001b[39;00m k \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/minerl/lib/python3.8/site-packages/torch/_tensor.py:680\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    679\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Keep track of the hidden state per episode/trajectory.\n",
    "# DataLoader provides unique id for each episode, which will\n",
    "# be different even for the same trajectory when it is loaded\n",
    "# up again\n",
    "episode_hidden_states = {}\n",
    "\n",
    "loss_sum = 0\n",
    "for batch_i, (batch_images, batch_actions, batch_episode_id) in enumerate(data_loader):\n",
    "    batch_loss = []\n",
    "\n",
    "    for image, action, episode_id in zip(batch_images, batch_actions, batch_episode_id):\n",
    "        if image is None and action is None:\n",
    "            # A work-item was done. Remove hidden state\n",
    "            if episode_id in episode_hidden_states:\n",
    "                removed_hidden_state = episode_hidden_states.pop(episode_id)\n",
    "                del removed_hidden_state\n",
    "            continue\n",
    "\n",
    "        agent_action = agent._env_action_to_agent(action, to_torch=True, check_if_null=True)\n",
    "        if agent_action is None:\n",
    "            # Action was null\n",
    "            continue\n",
    "\n",
    "        if episode_id not in episode_hidden_states:\n",
    "            episode_hidden_states[episode_id] = agent.controller.initial_state(1)\n",
    "        agent_state = episode_hidden_states[episode_id]\n",
    "\n",
    "        pred_action, new_agent_state = agent(image, agent_state)\n",
    "\n",
    "        # Make sure we do not try to backprop through sequence\n",
    "        # (fails with current accumulation)\n",
    "        new_agent_state = tree_map(lambda x: x.detach(), new_agent_state)\n",
    "        episode_hidden_states[episode_id] = new_agent_state\n",
    "\n",
    "        # Finally, update the agent to increase the probability of the\n",
    "        # taken action.\n",
    "        # Remember to take mean over batch losses\n",
    "        buttons_log_prob = torch.log_softmax(pred_action['buttons'], dim=-1)\n",
    "        buttons_loss = -buttons_log_prob[0, agent_action['buttons'].long()]\n",
    "        camera_log_prob = torch.log_softmax(pred_action['camera'], dim=-1)\n",
    "        camera_loss = -camera_log_prob[0, agent_action['camera'].long()]\n",
    "        loss = buttons_loss + camera_loss\n",
    "        batch_loss.append(loss)\n",
    "\n",
    "    batch_loss = torch.stack(batch_loss).mean()\n",
    "    print(batch_loss)\n",
    "    batch_loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(trainable_parameters, MAX_GRAD_NORM)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch_i % LOSS_REPORT_RATE == 0:\n",
    "        time_since_start = time.time() - start_time\n",
    "        print(f\"Time: {time_since_start:.2f}, Batches: {batch_i}, Avrg loss: {batch_loss:.4f}\")\n",
    "\n",
    "    if batch_i > MAX_BATCHES:\n",
    "        break\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_action['buttons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1970, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_action['buttons'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the negative log loss for the action\n",
    "log_prob = torch.log_softmax(pred_action['buttons'], dim=-1)\n",
    "loss = -log_prob[0, agent_action['buttons'].long()]\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.nll_loss(log_prob, agent_action['buttons'].squeeze(0), reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
