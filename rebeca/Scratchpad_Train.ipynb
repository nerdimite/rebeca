{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import minerl\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from rebeca import REBECA\n",
    "from data_loader import DataLoader\n",
    "from openai_vpt.lib.tree_util import tree_map\n",
    "\n",
    "# Originally this code was designed for a small dataset of ~20 demonstrations per task.\n",
    "# The settings might not be the best for the full BASALT dataset (thousands of demonstrations).\n",
    "# Use this flag to switch between the two settings\n",
    "USING_FULL_DATASET = True\n",
    "\n",
    "EPOCHS = 1 if USING_FULL_DATASET else 2\n",
    "# Needs to be <= number of videos\n",
    "BATCH_SIZE = 64 if USING_FULL_DATASET else 16\n",
    "# Ideally more than batch size to create\n",
    "# variation in datasets (otherwise, you will\n",
    "# get a bunch of consecutive samples)\n",
    "# Decrease this (and batch_size) if you run out of memory\n",
    "N_WORKERS = 100 if USING_FULL_DATASET else 20\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "LOSS_REPORT_RATE = 100\n",
    "\n",
    "# Tuned with bit of trial and error\n",
    "LEARNING_RATE = 0.000181\n",
    "# OpenAI VPT BC weight decay\n",
    "# WEIGHT_DECAY = 0.039428\n",
    "WEIGHT_DECAY = 0.0\n",
    "# KL loss to the original model was not used in OpenAI VPT\n",
    "KL_LOSS_WEIGHT = 1.0\n",
    "MAX_GRAD_NORM = 5.0\n",
    "\n",
    "MAX_BATCHES = 2000 if USING_FULL_DATASET else int(1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_model = \"data/VPT-models/foundation-model-1x.model\"\n",
    "in_weights = \"data/VPT-models/foundation-model-1x-net.weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "rebeca = REBECA(in_model, in_weights, \"data/memory.json\", device=DEVICE)\n",
    "rebeca.to(rebeca.device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoLoader:\n",
    "    def __init__(self, data_dir=\"data/MakeWaterfallTrain/\"):\n",
    "        self.load_expert_data(data_dir)\n",
    "        self.generator = self.load_demonstrations() # create a generator object\n",
    "\n",
    "    def load_expert_data(self, data_dir):\n",
    "        \"\"\"Load expert demonstrations from data_dir\"\"\"\n",
    "\n",
    "        unique_ids = glob.glob(os.path.join(data_dir, \"*.mp4\"))\n",
    "        unique_ids = list(set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))\n",
    "        unique_ids.sort()\n",
    "\n",
    "        self.demonstration_tuples = []\n",
    "        for unique_id in unique_ids:\n",
    "            video_path = os.path.abspath(os.path.join(data_dir, unique_id + \".mp4\"))\n",
    "            json_path = os.path.abspath(os.path.join(data_dir, unique_id + \".jsonl\"))\n",
    "            self.demonstration_tuples.append((unique_id, video_path, json_path))\n",
    "\n",
    "    def load_demonstrations(self):\n",
    "        \"\"\"Load expert demonstrations from demonstration tuples\"\"\"\n",
    "        _demonstration_tuples = self.demonstration_tuples\n",
    "\n",
    "        for unique_id, video_path, json_path in tqdm(\n",
    "            _demonstration_tuples, desc=\"Loading expert demonstrations\"\n",
    "        ):\n",
    "            video = self._load_video(video_path)\n",
    "            jsonl = self._load_jsonl(json_path)\n",
    "\n",
    "            yield video, jsonl\n",
    "\n",
    "    def _load_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def _load_jsonl(self, jsonl_path):\n",
    "        with open(jsonl_path) as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self # return the iterator object itself\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.generator) # return the next value from the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DemoLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee6225abf3e4e2781f569d14f96049f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for video, jsonl in data_loader:\n",
    "    rebeca.reset()\n",
    "    for frame, env_action in tqdm(zip(video, jsonl), total=len(video)):\n",
    "        pred_action = rebeca(frame)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3194, -0.2304,  0.1311,  ...,  0.0514,  0.1191,  0.3249]],\n",
       "        device='cuda:0', grad_fn=<ReshapeAliasBackward0>),\n",
       " tensor([[ 0.1215, -0.4680, -0.0967,  0.0766, -0.0943, -0.1302,  0.1931,  0.0405,\n",
       "           0.0266, -0.1493, -0.4175,  0.1956,  0.1271, -0.0098,  0.0247,  0.0814,\n",
       "          -0.1454, -0.1170, -0.2893, -0.1270, -0.0932, -0.0964, -0.3983, -0.2037,\n",
       "           0.0006,  0.1556,  0.2214, -0.0284, -0.1083,  0.2841, -0.1103, -0.1082,\n",
       "          -0.1439, -0.2897, -0.0180, -0.1113,  0.2394,  0.0360,  0.2240, -0.0315,\n",
       "           0.2324, -0.0670,  0.2696,  0.0841, -0.0163,  0.3667, -0.3701,  0.0058,\n",
       "          -0.1509, -0.4441,  0.0619, -0.0256, -0.0868, -0.1526,  0.0702,  0.2592,\n",
       "           0.1436, -0.2668, -0.0836,  0.1447,  0.3319,  0.0959,  0.1219, -0.0486,\n",
       "          -0.1173,  0.2014, -0.1207,  0.0532, -0.1319,  0.0784, -0.2219,  0.1838,\n",
       "          -0.1637,  0.0094, -0.2201, -0.0925,  0.0219, -0.1162, -0.2440,  0.0585,\n",
       "          -0.2819, -0.0107,  0.0316,  0.1053, -0.0370, -0.0633, -0.1916, -0.0440,\n",
       "          -0.0685,  0.2062, -0.2979,  0.0238,  0.2754,  0.3798,  0.2407,  0.0060,\n",
       "          -0.1730, -0.0769,  0.1683,  0.0490, -0.0773,  0.1205, -0.3238,  0.0274,\n",
       "           0.0067, -0.0761,  0.0364, -0.0286, -0.1210,  0.3206,  0.0424, -0.3071,\n",
       "           0.0197, -0.2443, -0.1676, -0.0403,  0.1352,  0.2882,  0.0492, -0.1418,\n",
       "           0.1055]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
